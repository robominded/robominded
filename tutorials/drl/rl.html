
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">	
    <!-- the following three lines to enable public comments -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/agate.min.css">

	<link rel="stylesheet" type="text/css" href="../../css/style.css">
    <link rel="stylesheet" type="text/css" href="../../css/header.css">
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Amiko&family=Michroma&family=Rajdhani&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Material+Icons|Gudea|Reem+Kufi|Rajdhani" rel="stylesheet">
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script> 
    <script> $(function(){ $("#include_header").load("../../pageElements/header.html"); }); </script> 
	<title>roboMind</title>
</head>

<body style="height: 100vh;">
<div id="include_header"></div>

	
	# Vanilla Bandits Problem
In vanilla bandits the agent is repeatedly faced with the problem of selecting an action amongst multiple actions. Each action selection yields a reward. The reward associated wich an action is usually stochastic in nature. The agent's goal is to select the action that gives the maximum expected reward. However, the reward distribution associated with each action is normally unknown to the agent. Therefore, the agent needs to sample the actions (i.e., select the actions) many times to estimate their rewards distributions. Once the reward distributions is estimated with confidence the agent can use this knowledge to select the best action and maximize the obtained reward. \\

* **Action-value:** is the expected reward of taking action $a$ (the true value of $a$). 
$$ q_*(a) = \mathbb{E}[R_t | A_t =a] \ \  \forall a \in \{1,\dots, k\} $$
* This value is usually unknown to the agent, and the agent goal is to estimate it. One way of estimating an action value is to select an action (sample an action) many times and compute the average reward.  
$$
Q_t(a) = \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}}
$$
* To reduce the memory footprint of computing the action-value estimate $Q_t(a)$, the following __Incremental update rule__ can be used instead:
$$ 
Q_{t+1} (a) = Q_{t} (a) + \alpha \big(R_t - Q_t(a)\big) \text{,}
$$
where $Q_{t}(a)$ is an estimate of $q_*(a)$ at time $t$. 

* Up to this point we provided the agent with means to estimate the value of an action. A naturally the agent should use this knowledge (action-value estimations) to make a decision (select an action). A logical action selection rule is select the action that gives the highest reward. 
$$
A_t = \operatorname*{argmax}_a Q_t(a) \text{,}
$$
where $A_t$ is the action taken at time $t$. However, being greedy always does not enable the agent to improve its estimation of other action values and therefore may never select the optimal action---the action that yield the maximum reward on the long run. This highlights the dilemma that the agent faces: when the agent should exploit its current knowledge, selecting the _current_ best action, and when it should explore other actions to improve its action values estimations and eventually select the "true" best action.

* **Epsilon-Greedy Action Selection Algorithm** provides a means for an agent to balance the exploration and exploitation trade-off to approach the optimal action selection. According to this algorithm, the agent should exploit its current knowledge (being greedy) with a probability $1- \epsilon$ and explore other actions with a probability $\epsilon$, where $\epsilon$ is $\lt 1$ 
$$
A_t \leftarrow \begin{cases}
      \operatorname*{argmax}_a  Q_t(a)  \ \ \  \text{with probability} \ \ 1- \epsilon \\
      a \textit{ ~ Uniform}(\{a_1, \dots, a_k \}) \ \ \  \text{with probability} \ \  \epsilon
\end{cases}
$$
<br>
<hr>

    <body>
</html>
